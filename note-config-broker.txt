**************************Note broker config*******************************

- broker.id: The broker id for this server. If unset, a unique broker id will be generated.To avoid conflicts between zookeeper generated broker id's 
                and user configured broker id's, generated broker ids start from reserved.broker.max.id + 1.
- log.dirs
- zookeeper.connect

- advertised.listeners: Listeners to publish to ZooKeeper for clients to use, if different than the listeners config property. In IaaS environments, 
        this may need to be different from the interface to which the broker binds. If this is not set, the value for listeners will be used. Unlike listeners, 
        it is not valid to advertise the 0.0.0.0 meta-address.

- auto.create.topics.enable: Enable auto creation of topic on the server

- background.threads: The number of threads to use for various background processing tasks

- log.flush.interval.messages: The number of messages accumulated on a log partition before messages are flushed to disk

- log.flush.interval.ms: The maximum time in ms that a message in any topic is kept in memory before flushed to disk. 
        If not set, the value in log.flush.scheduler.interval.ms is used

- min.insync.replicas: When a producer sets acks to "all" (or "-1"), min.insync.replicas specifies the minimum number of replicas that must acknowledge 
        a write for the write to be considered successful. If this minimum cannot be met, then the producer will raise an exception (either NotEnoughReplicas or 
        NotEnoughReplicasAfterAppend).When used together, min.insync.replicas and acks allow you to enforce greater durability guarantees. 
        A typical scenario would be to create a topic with a replication factor of 3, set min.insync.replicas to 2, and produce with acks of "all". 
        This will ensure that the producer raises an exception if a majority of replicas do not receive a write.

- offsets.commit.timeout.ms: Offset commit will be delayed until all replicas for the offsets topic receive the commit or this timeout is reached. 
        This is similar to the producer request timeout.

- offsets.retention.minutes: After a consumer group loses all its consumers (i.e. becomes empty) its offsets will be kept for this retention period 
        before getting discarded. For standalone consumers (using manual assignment), offsets will be expired after the time of last commit plus this retention period.

- request.timeout.ms: The configuration controls the maximum amount of time the client will wait for the response of a request. 
        If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted

**************************Note consumer config***********************************

- group.id: A unique string that identifies the consumer group this consumer belongs to. This property is required if the consumer uses either the group 
        management functionality by using subscribe(topic) or the Kafka-based offset management strategy.

- fetch_max_wait_ms (int) – The maximum amount of time in milliseconds the server will block before answering the fetch request 
        if there isn’t sufficient data to immediately satisfy the requirement given by fetch_min_bytes. Default: 500.

- fetch_max_bytes (int) – The maximum amount of data the server should return for a fetch request. This is not an absolute maximum, 
        if the first message in the first non-empty partition of the fetch is larger than this value, the message will still 
        be returned to ensure that the consumer can make progress. NOTE: consumer performs fetches to multiple brokers in parallel so memory usage will 
        depend on the number of brokers containing partitions for the topic. Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 MB).  

- request_timeout_ms (int) – Client request timeout in milliseconds. Default: 305000.

- auto_offset_reset (str) – A policy for resetting offsets on OffsetOutOfRange errors: ‘earliest’ will move to the oldest available message, 
        ‘latest’ will move to the most recent. Any other value will raise the exception. Default: ‘latest’.

- enable_auto_commit (bool) – If True , the consumer’s offset will be periodically committed in the background. Default: True.

- auto_commit_interval_ms (int) – Number of milliseconds between automatic offset commits, if enable_auto_commit is True. Default: 5000.

- heartbeat_interval_ms (int) – The expected time in milliseconds between heartbeats to the consumer coordinator when using Kafka’s group management facilities. 
        Heartbeats are used to ensure that the consumer’s session stays active and to facilitate rebalancing when new consumers join or leave the group. 
        The value must be set lower than session_timeout_ms, but typically should be set no higher than 1/3 of that value. 
        It can be adjusted even lower to control the expected time for normal rebalances. Default: 3000

**************************Note Kafka connect config******************************
- offset.storage.topic: The name of the Kafka topic where connector offsets are stored

- status.storage.topic: The name of the Kafka topic where connector and task status are stored

- session.timeout.ms: The timeout used to detect worker failures. The worker sends periodic heartbeats to indicate its liveness to the broker. 
        If no heartbeats are received by the broker before the expiration of this session timeout, 
        then the broker will remove the worker from the group and initiate a rebalance. Note that the value must be in 
        the allowable range as configured in the broker configuration by group.min.session.timeout.ms and group.max.session.timeout.ms.

- request.timeout.ms: The configuration controls the maximum amount of time the client will wait for the response of a request. 
        If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted.

************************Note Source Connector Configs****************************

- transforms: Aliases for the transformations to be applied to records.

- These and other related connector configuration properties can be changed to provide different behavior. 
        For example, the following configuration properties can be added to a connector configuration to setup error handling with multiple retries, 
        logging to the application logs and the my-connector-errors Kafka topic, 
        and tolerating all errors by reporting them rather than failing the connector task:

        # retry nhiều nhất 10 phút mot lần, chờ tối đa 30 giây giữa các lần thất bại liên tiếp
        errors.retry.timeout=600000
        errors.retry.delay.max.ms=30000

        # log error context along with application logs, but do not include configs and messages
        errors.log.enable=true
        errors.log.include.messages=false

        # produce error context into the Kafka topic
        errors.deadletterqueue.topic.name=my-connector-errors

        # Tolerate all errors.
        errors.tolerance=all

#################### Benchmarking #####################

1. Run a single producer client on a single server and measure the resulting throughput using
the available JMX metrics for the Kafka producer. Repeat the producer benchmarking test,
increasing the number of producer processes on the server in each interation to determine
the number of producer processes per server to achieve the highest throughput
2. Determine the baseline output performance profile for a given consumer in a similar way.
Run a single consumer client on a single server and repeat this test, increasing the number
of consumer processes per server to achieve the highest throughput
3. Run benchmark tests for different permutaions of configuration parameters that reflect your
services goal . Focus on a subset of configuration parameters

##################### Determining your service Goals #########################
Decide which service goals to optimize -> Configure Kafka cluster and clients -> Benchmark, monitor, and tune

It is important that you discuss the original  business use cases and main goals with your
team for the following two reason:
- Unable to maximize all goals at the same time. You may be familiar with the common trade-offset
in performance between throughput and latency and perhaps between durability and availibility
as well
- You must identify the service goals you want to optimize so you can tune your Kafka config
parameters to achieve them:
        + Do you want to optimize for high throughput, which is the rate that data is moved
        from producers to brokers or brokers to consumers.
        + Do you want to optimize for low latency, which is the time elapsed moving messages
        end to end (producers->broker->consumers)?
        + Do you want to optimize for high durability, which guarantees that committted
        messages will not be lost ?
        + Do you want to optimize for high availibility, which minimizes downtime in case of
        unexpected failures ?

#################### Optimizing performance for apache Kafka ######################
1. Optimizing for Throughput

To optimize for Throughput, producers and consumers must move as much data as possible within
a given amount of time. For high throughput, try maximizing the rate at which the data moves.

a. Number of partitions
There are trade-offs to increasing the number of partitions. Be sure to choose the partition 
count based on producer throughput and consumer throughput, and bendchmark performance in your
environments
link:(https://www.confluent.io/blog/how-choose-number-topics-partitions-kafka-cluster/?_ga=2.148803572.2108300758.1625038063-218728915.1618299640&_gac=1.89329257.1624003597.CjwKCAjwiLGGBhAqEiwAgq3q_tTl1wJIkR7n1fwNL4Z6xR6WA0ZiU-jfH_wHySXWGFiyCf8oEBRvnBoC-fgQAvD_BwE)

b. Batching Meassages
With Batching of Kafka producers, can batch messages going to the same partition, which mean they
collect multiple messages to send together in a single request. Large batch size help reduce load
on producers and the broker CPU overhead to process request.
Parameter to note:
        -batch.size : increase the maximum size in bytes of each message batch
        -linger.ms : to have the producer wait longer before sending. The delay allows the 
                     producer to wait for the batch to reach the configured batch.size

c. Compression
Which means many bits can be sent as fewer bits. Enable Compression by config compression.type with:
        - lz4
        - snappy
        - zstd
        - gzip
        - lz4 (recommend for performance)

d. Producer acks
The sooner a producer  receives a response, the sooner the producer can send the next message,
which generally results in higher throughput.  So producers can set the configuration parameter
acks to specify the number of acknowledgments the leader broker must have received before
responding to the producer with an acknowledgment. Set acks=1

e. Memory Allocation
Kafka producers automatically allocate memory for the Java client to store unsent messages. You can
adjust how much memory is allocated with the configuration parameter buffer.memory. If you don't 
have a lot of partitions, you may not need to adjust this at all.

f. Consumer fetching
Another way to optimize for throughput is adjust how much data consumers receive from each fetch
from the leader broker. You can increase how much data the consumers get from the leader for each 
fetch request by increasing the configuration parameter fetch.min.bytes

g summary
- Producer
batch.size: increase to 100000–200000 (default 16384)
linger.ms: increase to 10–100 (default 0)
compression.type=lz4 (default none, for example, no compression)
acks=1 (default 1)
buffer.memory: increase if there are a lot of partitions (default 33554432)
- Consumer
fetch.min.bytes: increase to ~100000 (default 1)

2 . Optimizing for latency
a. Producer acks
there is a trade-off for an increased number of partitions, and that's increased latency. It may
take longer to replicate several partitions shared between each pair of brokers and consequently take
longer for messageses to be considered commited. No message can be consumed until it is committed
so this can ultimately increase end-to-end latency

b. Batching Messages
Producer automatically batch messages, which means they collect messages to send together. If 
there is less time given waiting for those batches to fill, then there is less latency producing
data. By default, the producer is tuned for low latency and the configuration parameter linger.ms
is set to 0 - the producer sends data as soon as it has data to send.

c. Compression
compression.type=none

d. Producer acks
Depending on your application requirement, you can set acks=0 so that the producer won't wait 
for a response for a producer request from the broker, but then messages can potentially get 
lost without the producer knowing

e. Consumer fetching
fetch.min.bytes=1

f. Summary
- Producer
linger.ms=0 (default 0)
compression.type=none (default none, meaning no compression)
acks=1 (default 1)

- Consumer
fetch.min.bytes=1 (default 1)

3. Optimizing for Durability 
Summary of Configurations for Optimizing Durability
- Producer 
replication.factor=3
acks=all (default 1)
enable.idempotence=true (default false), to prevent duplicate messages and out-of-order messages
max.in.flight.requests.per.connection=1 (default 5), to prevent out of order messages when not using an idempotent producer

- Consumer
enable.auto.commit=false (default true)
isolation.level=read_committed (when using EOS transactions)

4. Optimizing for Availability
a. Consumer Failures
Consumers in a consumer group can share processing load. If a consumer unexpectedly fails, Kafka
can detect the failure and rebalance the partitions amongst the remaining consumers in the consumer
group. The consumer failures can be hard failures (ex: SIGKILL) or soft failures (ex: expired
session timeouts). These failures can be detected either when consumers fail to send hearbeats or
when they fail to send poll() calls, the configuration parameter session.timeout.ms dictates
the timeout used to detect failed heartbeats

session.timeout.ms: increase (default 10000)